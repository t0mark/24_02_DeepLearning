{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff89c544",
   "metadata": {},
   "source": [
    "## 1. Data preparation: data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5cc1a0-a6bd-41b2-89c8-df469484e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e5fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(20),          # 랜덤 회전\n",
    "    transforms.RandomResizedCrop(224),      # 사이즈 맞추기\n",
    "    transforms.RandomHorizontalFlip(),      # 좌우 뒤집기\n",
    "    transforms.ToTensor(),                  # 텐서로 변환\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # 정규화\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),                # 이미지 크기 조정\n",
    "    transforms.CenterCrop(224),            # 중앙 크롭\n",
    "    transforms.ToTensor(),                 # 텐서로 변환\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # 정규화\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb2f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoDataset(Dataset):\n",
    "    def __init__(self, path, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.transforms = transforms\n",
    "\n",
    "        for dir in ['bo', 'not_bo']:\n",
    "            label = 1 if dir == 'bo' else 0\n",
    "            dir_path = os.path.join(path, dir)\n",
    "            for img in os.listdir(dir_path):\n",
    "                self.x.append(os.path.join(dir_path, img))\n",
    "                self.y.append(label)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.x[idx]).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms:\n",
    "            x_tensor = self.transforms(image)\n",
    "        else:\n",
    "            x_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "        y_tensor = torch.tensor(self.y[idx])\n",
    "        \n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95fbc6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoDataLoader:\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        self.num_samples = len(dataset)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)  # 데이터 셔플링\n",
    "        self.current_index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= self.num_samples:\n",
    "            raise StopIteration\n",
    "        \n",
    "        batch_indices = self.indices[self.current_index:self.current_index+self.batch_size]\n",
    "        batch = [self.dataset[i] for i in batch_indices]\n",
    "        self.current_index += self.batch_size\n",
    "\n",
    "        batch_x, batch_y = zip(*batch)\n",
    "        return torch.stack(batch_x), torch.tensor(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd2ba48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './hw3_bo_and_notbo/train'\n",
    "valid_path = './hw3_bo_and_notbo/valid'\n",
    "\n",
    "train_data = BoDataset(train_path, transforms=train_transforms)\n",
    "val_data = BoDataset(valid_path, transforms=val_transforms)\n",
    "\n",
    "train_loader = BoDataLoader(train_data, batch_size=20, shuffle=True)\n",
    "val_loader = BoDataLoader(val_data, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de04cc53",
   "metadata": {},
   "source": [
    "## 2. Constructing a neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6499f04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "resnet.fc = nn.Linear(in_features=resnet.fc.in_features, out_features=2)\n",
    "resnet = resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e21e3",
   "metadata": {},
   "source": [
    "## 3. Loss function and optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bc0b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd850523",
   "metadata": {},
   "source": [
    "## 4. Training of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b2758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  , Loss: 3.49159674346447\n",
      "Epoch: 2  , Loss: 2.8907951414585114\n",
      "Epoch: 3  , Loss: 1.8670158237218857\n",
      "Epoch: 4  , Loss: 1.9716014564037323\n",
      "Epoch: 5  , Loss: 1.1646679937839508\n",
      "Epoch: 6  , Loss: 1.0768968760967255\n",
      "Epoch: 7  , Loss: 0.8807556256651878\n",
      "Epoch: 8  , Loss: 0.6507934927940369\n",
      "Epoch: 9  , Loss: 0.594440970569849\n",
      "Epoch: 10  , Loss: 0.5289658010005951\n",
      "Epoch: 11  , Loss: 0.5132788326591253\n",
      "Epoch: 12  , Loss: 0.38628101721405983\n",
      "Epoch: 13  , Loss: 0.39037197828292847\n",
      "Epoch: 14  , Loss: 0.4037373121827841\n",
      "Epoch: 15  , Loss: 0.41993197426199913\n",
      "Epoch: 16  , Loss: 0.4821723308414221\n",
      "Epoch: 17  , Loss: 0.3692601229995489\n",
      "Epoch: 18  , Loss: 0.3765437211841345\n",
      "Epoch: 19  , Loss: 0.2877541985362768\n",
      "Epoch: 20  , Loss: 0.24791929125785828\n",
      "Epoch: 21  , Loss: 0.26789115369319916\n",
      "Epoch: 22  , Loss: 0.30350164137780666\n",
      "Epoch: 23  , Loss: 0.24732948187738657\n",
      "Epoch: 24  , Loss: 0.19429335556924343\n",
      "Epoch: 25  , Loss: 0.27366623375564814\n",
      "Epoch: 26  , Loss: 0.23788858205080032\n",
      "Epoch: 27  , Loss: 0.16176907951012254\n",
      "Epoch: 28  , Loss: 0.11422612611204386\n",
      "Epoch: 29  , Loss: 0.27761267498135567\n",
      "Epoch: 30  , Loss: 0.19671469926834106\n"
     ]
    }
   ],
   "source": [
    "para = []\n",
    "for epoch in range(30):\n",
    "    loss_val = 0\n",
    "    for itr, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        pred = resnet(inputs)\n",
    "        loss = loss_function(pred, labels)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        para.append(resnet.state_dict())\n",
    "\n",
    "    print(\"Epoch:\", epoch+1, \" , Loss:\", loss_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c287c4",
   "metadata": {},
   "source": [
    "## 5. Prediction and Evaluation for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658df28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(net, loader):\n",
    "  pred_list = []\n",
    "  label_list = []\n",
    "  for itr, data in enumerate(loader):\n",
    "    inputs, labels = data\n",
    "\n",
    "    pred_test = net(inputs)\n",
    "    pred_category = torch.argmax(pred_test, dim=1)\n",
    "\n",
    "    pred_list = pred_list + list(pred_category)\n",
    "    label_list = label_list + list(labels)\n",
    "\n",
    "  accu = np.mean(np.array(pred_list) == np.array(label_list))\n",
    "  return accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46dc51f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for model 1: 0.8333333333333334\n",
      "Test for model 2: 0.8333333333333334\n",
      "Test for model 3: 0.8333333333333334\n",
      "Test for model 4: 0.8333333333333334\n",
      "Test for model 5: 0.8333333333333334\n",
      "Test for model 6: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "if device != 'cpu':\n",
    "    rsenet = resnet.to('cpu')\n",
    "\n",
    "max_accu = 0\n",
    "best_net = None\n",
    "\n",
    "for i, tmp_param in enumerate(para):\n",
    "    resnet.load_state_dict(tmp_param)\n",
    "    accu = cal_accuracy(resnet, val_loader)\n",
    "    print(f\"Test for model {i+1}: {accu}\")\n",
    "\n",
    "if accu > max_accu:\n",
    "    max_accu = accu\n",
    "    best_net = tmp_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6d43923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9928057553956835\n",
      "valid accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy: \"+ str(cal_accuracy(resnet, train_loader)))\n",
    "print(\"valid accuracy: \"+ str(cal_accuracy(resnet, val_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e7641",
   "metadata": {},
   "source": [
    "## 보고서 내용\n",
    "\n",
    "### 데이터셋\n",
    "\n",
    "1. 이미지 데이터를 처리하기 위한 데이터셋 클래스 정의\n",
    "2. 각 이미지 경로를 찾아서 라벨링\n",
    "```\n",
    "for dir in ['bo', 'not_bo']:\n",
    "    label = 1 if dir == 'bo' else 0\n",
    "    dir_path = os.path.join(path, dir)\n",
    "    for img in os.listdir(dir_path):\n",
    "        self.x.append(os.path.join(dir_path, img))\n",
    "        self.y.append(label)\n",
    "```\n",
    "3. 추가적인 데이터를 위해 데이터 증강\n",
    "```python\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(20),          # 랜덤 회전\n",
    "    transforms.RandomResizedCrop(224),      # 사이즈 맞추기\n",
    "    transforms.RandomHorizontalFlip(),      # 좌우 뒤집기\n",
    "    transforms.ToTensor(),                  # 텐서로 변환\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # 정규화\n",
    "])\n",
    "```\n",
    "4. 모델 사용 및 이미지 크기 통일을 위해 224x224로 사이즈 조정\n",
    "\n",
    "### 데이터로더\n",
    "1. 셔플 기능 구현\n",
    "```\n",
    "if self.shuffle:\n",
    "    np.random.shuffle(self.indices)  # 데이터 셔플링\n",
    "```\n",
    "2. 배치 사이즈 별로 같은 배열에 포함하여 반환\n",
    "3. 배치 크기 별로 batch_x, batch_y 형태로 반환\n",
    "```\n",
    "batch_indices = self.indices[self.current_index:self.current_index+self.batch_size]\n",
    "batch = [self.dataset[i] for i in batch_indices]\n",
    "self.current_index += self.batch_size\n",
    "\n",
    "batch_x, batch_y = zip(*batch)\n",
    "return torch.stack(batch_x), torch.tensor(batch_y)\n",
    "```\n",
    "\n",
    "### 네트워크 모델\n",
    "\n",
    "1. ResNet 사용\n",
    "2. pre-trained parameter 사용\n",
    "3. 출력층만 학습에 이용\n",
    "```\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "resnet.fc = nn.Linear(in_features=resnet.fc.in_features, out_features=2)\n",
    "```\n",
    "\n",
    "### 손실함수, 최적화\n",
    "\n",
    "1. 손실함수: CrossEntropy\n",
    "2. 최적화: Adam\n",
    "\n",
    "### 훈련 과정\n",
    "1. 5 epoch 마다 parameter 저장 → 최선의 파라미터 선택\n",
    "\n",
    "### 파일 구조\n",
    "HW3/  \n",
    "├── train/  \n",
    "│   ├── bo  \n",
    "│   │   ├── img1.jpg  \n",
    "│   │   ├── ...  \n",
    "│   ├── not_bo  \n",
    "│   │   ├── img2.jpg  \n",
    "│   │   ├── ...  \n",
    "├── valid/  \n",
    "│   ├── bo  \n",
    "│   │   ├── img3.jpg  \n",
    "│   │   ├── ...  \n",
    "│   ├── not_bo  \n",
    "│   │   ├── img4.jpg  \n",
    "│   │   ├── ...  \n",
    "├── hw3_202011505.ipynb  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
